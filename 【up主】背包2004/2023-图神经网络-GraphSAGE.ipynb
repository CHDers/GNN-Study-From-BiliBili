{"cells":[{"cell_type":"markdown","metadata":{"collapsed":false,"id":"18608840882D48C99AF17747811DF15C","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["\n","\n","## GraphSAGE 核心思想\n","\n","GraphSAGE的核心：GraphSAGE不是试图学习一个图上所有node的embedding，而是学习一个为每个node产生embedding的映射。\n","\n","\n","论文中提出的方法称为GraphSAGE, SAGE指的是 Sample and Aggregate，不是对每个顶点都训练一个单独的embeddding向量，而是训练了一组aggregator functions，这些函数学习如何从一个顶点的局部邻居聚合特征信息。每个聚合函数从一个顶点的不同的hops或者说不同的搜索深度聚合信息。测试或是推断的时候，使用训练好的系统，通过学习到的聚合函数来对完全未见过的顶点生成embedding。\n","\n","![](https://img-blog.csdnimg.cn/img_convert/beebbdcaf3b468efee9b9ffdb530c3dd.png)\n","上面是为红色的目标节点生成embedding的过程。k表示距离目标节点的搜索深度，k=1就是目标节点的相邻节点，k=2表示目标节点相邻节点的相邻节点。\n","对于上图中的例子：\n","- 第一步是采样，k=1采样了3个节点，对k=2采用了5个节点；\n","- 第二步是聚合邻居节点的信息，获得目标节点的embedding；\n","- 第三步是使用聚合得到的信息，也就是目标节点的embedding,来预测图中想预测的信息;\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["此课件来自如下链接的整合\n","\n","https://zhuanlan.zhihu.com/p/410407148\n","\n","https://zhuanlan.zhihu.com/p/512929377\n","\n","https://blog.csdn.net/weixin_44027006/article/details/116888648\n","\n","https://www.heywhale.com/mw/project/608538b1c7cba5001752d619"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## graphSAGE 源码"]},{"cell_type":"markdown","metadata":{},"source":["### 采样"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","import numpy as np\n","\n","def sampling(src_nodes, sample_num, neighbor_table):\n","    \"\"\"根据源节点采样指定数量的邻居节点，注意使用的是有放回的采样；\n","    某个节点的邻居节点数量少于采样数量时，采样结果出现重复的节点\n","\n","    Arguments:\n","        src_nodes {list, ndarray} -- 源节点列表\n","        sample_num {int} -- 需要采样的节点数\n","        neighbor_table {dict} -- 节点到其邻居节点的映射表\n","\n","    Returns:\n","        np.ndarray -- 采样结果构成的列表\n","    \"\"\"\n","    results = []\n","    for sid in src_nodes:\n","        # # 从节点的邻居中进行有放回地进行采样\n","        # res = np.random.choice(neighbor_table[sid], size=(sample_num,))\n","        # results.append(res)\n","\n","\n","        if len(neighbor_table[sid]) >= sample_num:\n","            res = np.random.choice(neighbor_table[sid], size=(sample_num,),replace=False)\n","        else:\n","            res = np.random.choice(neighbor_table[sid], size=(sample_num,),replace=True)\n","        results.append(res)\n","    return np.asarray(results).flatten()\n","\n","\n","def multihop_sampling(src_nodes, sample_nums, neighbor_table):\n","    \"\"\"根据源节点进行多阶采样\n","\n","    Arguments:\n","        src_nodes {list, np.ndarray} -- 源节点id\n","        sample_nums {list of int} -- 每一阶需要采样的个数\n","        neighbor_table {dict} -- 节点到其邻居节点的映射\n","\n","    Returns:\n","        [list of ndarray] -- 每一阶采样的结果\n","    \"\"\"\n","    sampling_result = [src_nodes]\n","    # print(\"sampling result = \", sampling_result)\n","    # print(\"sample_nums = \", sample_nums)\n","\n","    for k, hopk_num in enumerate(sample_nums):\n","        # print(\"sampling_result[k] = \", sampling_result[k])\n","        hopk_result = sampling(sampling_result[k], hopk_num, neighbor_table)\n","        sampling_result.append(hopk_result)\n","    return sampling_result\n"]},{"cell_type":"markdown","metadata":{},"source":["### 聚合与训练"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.init as init\n","\n","\n","class NeighborAggregator(nn.Module):\n","    def __init__(self, input_dim, output_dim,\n","                    use_bias=False, aggr_method=\"mean\"):\n","        \"\"\"聚合节点邻居\n","\n","        Args:\n","            input_dim: 输入特征的维度\n","            output_dim: 输出特征的维度\n","            use_bias: 是否使用偏置 (default: {False})\n","            aggr_method: 邻居聚合方式 (default: {mean})\n","        \"\"\"\n","        super(NeighborAggregator, self).__init__()\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.use_bias = use_bias\n","        self.aggr_method = aggr_method\n","        self.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))\n","        if self.use_bias:\n","            self.bias = nn.Parameter(torch.Tensor(self.output_dim))\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        init.kaiming_uniform_(self.weight)\n","        if self.use_bias:\n","            init.zeros_(self.bias)\n","\n","    def forward(self, neighbor_feature):\n","        if self.aggr_method == \"mean\":\n","            aggr_neighbor = neighbor_feature.mean(dim=1)\n","        elif self.aggr_method == \"sum\":\n","            aggr_neighbor = neighbor_feature.sum(dim=1)\n","        elif self.aggr_method == \"max\":\n","            aggr_neighbor = neighbor_feature.max(dim=1)\n","        else:\n","            raise ValueError(\"Unknown aggr type, expected sum, max, or mean, but got {}\"\n","                                .format(self.aggr_method))\n","\n","        neighbor_hidden = torch.matmul(aggr_neighbor, self.weight)\n","        if self.use_bias:\n","            neighbor_hidden += self.bias\n","\n","        return neighbor_hidden\n","\n","    def extra_repr(self):\n","        return 'in_features={}, out_features={}, aggr_method={}'.format(\n","            self.input_dim, self.output_dim, self.aggr_method)\n","\n","\n","class SageGCN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim,\n","                    activation=F.relu,\n","                    aggr_neighbor_method=\"mean\",\n","                    aggr_hidden_method=\"sum\"):\n","        \"\"\"SageGCN层定义\n","\n","        Args:\n","            input_dim: 输入特征的维度\n","            hidden_dim: 隐层特征的维度，\n","                当aggr_hidden_method=sum, 输出维度为hidden_dim\n","                当aggr_hidden_method=concat, 输出维度为hidden_dim*2\n","            activation: 激活函数\n","            aggr_neighbor_method: 邻居特征聚合方法，[\"mean\", \"sum\", \"max\"]\n","            aggr_hidden_method: 节点特征的更新方法，[\"sum\", \"concat\"]\n","        \"\"\"\n","        super(SageGCN, self).__init__()\n","        assert aggr_neighbor_method in [\"mean\", \"sum\", \"max\"]\n","        assert aggr_hidden_method in [\"sum\", \"concat\"]\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.aggr_neighbor_method = aggr_neighbor_method\n","        self.aggr_hidden_method = aggr_hidden_method\n","        self.activation = activation\n","        self.aggregator = NeighborAggregator(input_dim, hidden_dim,\n","                                                aggr_method=aggr_neighbor_method)\n","        self.weight = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        init.kaiming_uniform_(self.weight)\n","\n","    def forward(self, src_node_features, neighbor_node_features):\n","        neighbor_hidden = self.aggregator(neighbor_node_features)\n","        self_hidden = torch.matmul(src_node_features, self.weight)\n","\n","        if self.aggr_hidden_method == \"sum\":\n","            hidden = self_hidden + neighbor_hidden\n","        elif self.aggr_hidden_method == \"concat\":\n","            hidden = torch.cat([self_hidden, neighbor_hidden], dim=1)\n","        else:\n","            raise ValueError(\"Expected sum or concat, got {}\"\n","                                .format(self.aggr_hidden))\n","        if self.activation:\n","            return self.activation(hidden)\n","        else:\n","            return hidden\n","\n","    def extra_repr(self):\n","        output_dim = self.hidden_dim if self.aggr_hidden_method == \"sum\" else self.hidden_dim * 2\n","        return 'in_features={}, out_features={}, aggr_hidden_method={}'.format(\n","            self.input_dim, output_dim, self.aggr_hidden_method)\n","\n","\n","class GraphSage(nn.Module):\n","    def __init__(self, input_dim, hidden_dim,\n","                    num_neighbors_list):\n","        super(GraphSage, self).__init__()\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_neighbors_list = num_neighbors_list\n","        self.num_layers = len(num_neighbors_list)\n","        self.gcn = nn.ModuleList()\n","        self.gcn.append(SageGCN(input_dim, hidden_dim[0]))\n","        for index in range(0, len(hidden_dim) - 2):\n","            self.gcn.append(SageGCN(hidden_dim[index], hidden_dim[index + 1]))\n","        self.gcn.append(SageGCN(hidden_dim[-2], hidden_dim[-1], activation=None))\n","\n","    def forward(self, node_features_list):\n","        hidden = node_features_list\n","\n","        for l in range(self.num_layers):\n","            # print(f\"========= 第 {l} 层 =========\")\n","            next_hidden = []\n","            gcn = self.gcn[l]\n","            for hop in range(self.num_layers - l):\n","            # print(\"self.num_layers - l \" , self.num_layers - l-1)\n","            # for hop in range(self.num_layers - l-1,l,-1):\n","            #     print(f\"======== hop {hop} ============ \" )\n","                src_node_features = hidden[hop]\n","                src_node_num = len(src_node_features)\n","                # print(\" src_node_num = \", src_node_features.shape)\n","\n","                neighbor_node_features = hidden[hop + 1].view((src_node_num, self.num_neighbors_list[hop], -1))\n","                # print(\" neighbor_node_features = \", neighbor_node_features.shape)\n","\n","                h = gcn(src_node_features, neighbor_node_features)\n","                # print(\" after gcn h = \", h.shape)\n","\n","                next_hidden.append(h)\n","            hidden = next_hidden\n","            # print(\"hidden shape = \",len(hidden))\n","        return hidden[0]\n","\n","    def extra_repr(self):\n","        return 'in_features={}, num_neighbors_list={}'.format(\n","            self.input_dim, self.num_neighbors_list\n","        )\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## 基于pyg的graphsage实现"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os.path as osp\n","import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","from torch_geometric.datasets import Planetoid\n","from tqdm import tqdm\n","\n","from torch_geometric.loader import NeighborSampler\n","from torch_geometric.nn import SAGEConv"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["dataset =Planetoid(root=r\"./data\",name='Cora')\n","data = dataset[0]\n","num_nodes_list = torch.arange(data.num_nodes)\n","train_idx = num_nodes_list[data['train_mask']]"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["140"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["len(train_idx)"]},{"cell_type":"markdown","metadata":{},"source":["### NeighborSampler\n","\n","https://blog.csdn.net/qq_40671063/article/details/126803861"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["train_loader = NeighborSampler(data.edge_index, node_idx=train_idx,\n","                               sizes=[15, 10, 5], batch_size=70,\n","                               shuffle=True, num_workers=12)\n","subgraph_loader = NeighborSampler(data.edge_index, node_idx=None, sizes=[-1],\n","                                  batch_size=256, shuffle=False,\n","                                  num_workers=12)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["2"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["len(train_loader)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[EdgeIndex(edge_index=tensor([[ 70,  71,  72,  ..., 537, 789, 889],\n","        [  0,   0,   0,  ..., 863, 863, 863]]), e_id=tensor([1734, 5180, 8373,  ..., 8329, 3680, 1412]), size=(1431, 864)), EdgeIndex(edge_index=tensor([[ 70,  71,  72,  ..., 199, 200, 863],\n","        [  0,   0,   0,  ..., 326, 326, 326]]), e_id=tensor([1734, 5180, 8373,  ..., 8660, 8862, 5993]), size=(864, 327)), EdgeIndex(edge_index=tensor([[ 70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n","          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n","          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n","         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n","          20, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 115, 137,\n","         138, 139, 140, 141, 142, 143, 144, 145,  47, 146, 147, 148, 149, 150,\n","         151, 152, 153, 154, 155, 156, 157,  84, 158, 159,  83,  84,  10, 127,\n","         128, 160, 161, 162, 117, 163, 164, 165, 166, 167, 168, 169, 170,  72,\n","         171, 172, 173, 174,  52, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n","         184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n","         198, 199, 200, 201, 202, 203, 204, 205, 206,  38,  69, 115, 117, 207,\n","         208, 209, 210,  37, 115, 211, 212, 213, 214, 215, 216,  56, 190, 217,\n","         218, 219, 220,  84, 221, 222, 223, 224, 225, 226, 227, 228, 229, 108,\n","         149, 153, 156, 216, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n","         240, 241, 242, 115, 163, 243, 244, 245,  16, 246, 247, 117, 163, 210,\n","         248, 249, 250, 251, 252, 253, 142, 143, 254, 255, 256, 257, 258, 259,\n","         260, 261, 262, 263, 264, 265, 266,  24, 267, 268, 269, 270, 178, 271,\n","         272, 273, 274, 275, 276, 277, 278,  41, 217, 219, 220, 279, 280, 281,\n","         282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292,  70, 293, 294,\n","         295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 151, 307,\n","         308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n","         322, 323,  37, 253, 324, 325, 326],\n","        [  0,   0,   0,   0,   1,   1,   1,   1,   1,   1,   1,   1,   1,   2,\n","           2,   2,   3,   3,   3,   3,   4,   4,   4,   4,   4,   4,   5,   5,\n","           5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   6,\n","           6,   6,   6,   7,   7,   7,   7,   8,   8,   8,   8,   8,   9,   9,\n","          10,  10,  10,  10,  10,  11,  11,  12,  12,  12,  12,  12,  13,  13,\n","          13,  14,  15,  15,  15,  15,  15,  15,  16,  16,  16,  16,  16,  16,\n","          16,  16,  16,  16,  16,  16,  17,  18,  18,  18,  19,  19,  20,  20,\n","          20,  20,  20,  20,  21,  21,  21,  21,  21,  21,  21,  21,  22,  23,\n","          23,  23,  23,  23,  24,  24,  24,  24,  24,  24,  24,  25,  25,  26,\n","          26,  26,  26,  27,  27,  27,  28,  29,  30,  30,  31,  31,  32,  33,\n","          34,  34,  34,  35,  35,  35,  36,  36,  36,  37,  37,  37,  37,  37,\n","          37,  37,  37,  38,  38,  38,  38,  38,  38,  39,  40,  41,  41,  41,\n","          41,  41,  41,  42,  42,  42,  42,  42,  42,  42,  42,  42,  42,  43,\n","          43,  43,  43,  43,  43,  43,  43,  43,  43,  44,  44,  44,  44,  44,\n","          44,  44,  45,  46,  46,  46,  46,  46,  47,  47,  47,  48,  48,  48,\n","          48,  48,  48,  48,  48,  48,  49,  49,  49,  49,  49,  49,  49,  49,\n","          49,  49,  50,  50,  50,  51,  51,  52,  52,  52,  52,  52,  53,  53,\n","          54,  54,  54,  54,  55,  55,  55,  56,  56,  56,  56,  57,  57,  58,\n","          58,  58,  58,  58,  59,  59,  60,  60,  61,  61,  61,  62,  62,  62,\n","          62,  63,  64,  64,  64,  65,  65,  65,  65,  65,  65,  66,  67,  67,\n","          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  68,\n","          68,  68,  69,  69,  69,  69,  69]]), e_id=tensor([ 1734,  5180,  8373,  9450,  1157,  2397,  2786,  3606,  4092,  8423,\n","         8745,  8747,  8752,  4298,  5260,  6994,  1670,  1837,  6361,  7528,\n","         1885,  2833,  5532,  9146,  9149,  9152,  3417,  2764,  2942,  6079,\n","         8492,  2963,  1875,  5058,  9015,  8568,  9042,  9086,  6459,  9107,\n","         9110,  1122,  2135,  7535,  8071,  1213,  2005,  6392,  8598,  4454,\n","         5812,  8490,  8977,  9023,  6014, 10304,   295,  1148,  9189,  9193,\n","         9205,  1065, 10204,  1986,  2120,  3256,  7496,  9825,  1216,   976,\n","         7184,   883,   684,  3735,  6059,  6544,  9809,  9814,   231,   885,\n","         1353,  2633,  3114,  3155,  3250,  4202,  4515,  8121,  8318,  8320,\n","         3305,  5262,  1587,  8134,  4299,  5263,   322,  9188,  9192,  2381,\n","         9184,  9200,  6391,   476,  4867,  5758,  6193,  7601,  7624,  9326,\n","         3784,  8371,   723,  8572,  8585, 10456,    71,  5231,  6020,  6068,\n","         6487,  8040,  9636,  6326,  9675,   779,  2421,  2599,  7789,  1091,\n","         1136,  8218, 10349,  6297,  3559,  7472,  3322,  8969,  5716,  9999,\n","          623,  8659,  8860,  4747,  5156,  5655,  1054,  4355,  8479,   454,\n","           58,  1217,  6395,  2666,  3600,  7214,  8440,   631,  1211,  1990,\n","         2491,  7179,  7260,  1997,  9857,   572, 10347,    88,   576,  9981,\n","         9984,  5261,   464,  2363,  2496,  2518,  4136,  5003,  6454,  6920,\n","        10422,  6458,  3115,  4516,  8321,  9858,   987,  2228,  2528,  5020,\n","         9863,  2401,  3212,  3814,  4523,  7769,  9317,  9320, 10222,  1214,\n","          477,  5454,  6387, 10054,   202,   132,  6039,  6394,   478,  8439,\n","         1614,  1854,  6612,  7196,  7233,  8611,  6060,  6545,  1058,  3505,\n","         4230,  4257,  4519,  5561,  6270, 10001,  6526,  8923,  9135,  6317,\n","         8505,   139,  4816,  6803,  6813,  9246,  6488,  4451,  3773,  6483,\n","         9926, 10402,  2569,  7565, 10306,   436,    89,  9982,  9985,  1620,\n","         3104,  4158,  9360,  9367,  9706,  9707,  2878, 10363,  2468,  8669,\n","         2214,  3024,  7646,  1733,  3421,  3423,  4535,  1646,  6256, 10379,\n","        10541,  1545,  2617,  6089,  6150,  7714,  8018,  6018,  3251,  6868,\n","         7518,  5536,  1192,  7756,  7017,  5079,  5921,  8294,  8296,  1675,\n","         6959,  8292,  7640,  2085,  3432,  5446,   630,  8610,  6190,  7243,\n","         8866]), size=(327, 70))]\n","70\n","tensor([  86,   33,   53,  ..., 1934, 1432, 2610])\n"]}],"source":["for batch_size, n_id, adjs in train_loader:\n","    print(adjs)\n","    print(batch_size)\n","    print(n_id)\n","    break"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["1431"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["len(n_id)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([  86,   33,   53,  ..., 1934, 1432, 2610])"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["n_id"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(86)\n","tensor(33)\n","tensor(53)\n","tensor(56)\n","tensor(51)\n","tensor(95)\n","tensor(32)\n","tensor(112)\n","tensor(52)\n","tensor(83)\n","tensor(84)\n","tensor(117)\n","tensor(120)\n","tensor(138)\n","tensor(7)\n","tensor(43)\n","tensor(55)\n","tensor(136)\n","tensor(68)\n","tensor(72)\n","tensor(75)\n","tensor(102)\n","tensor(115)\n","tensor(14)\n","tensor(39)\n","tensor(46)\n","tensor(41)\n","tensor(8)\n","tensor(127)\n","tensor(31)\n","tensor(92)\n","tensor(131)\n","tensor(50)\n","tensor(106)\n","tensor(135)\n","tensor(78)\n","tensor(80)\n","tensor(139)\n","tensor(103)\n","tensor(62)\n","tensor(57)\n","tensor(99)\n","tensor(59)\n","tensor(65)\n","tensor(121)\n","tensor(3)\n","tensor(124)\n","tensor(60)\n","tensor(133)\n","tensor(89)\n","tensor(108)\n","tensor(100)\n","tensor(22)\n","tensor(82)\n","tensor(16)\n","tensor(0)\n","tensor(122)\n","tensor(54)\n","tensor(20)\n","tensor(9)\n","tensor(79)\n","tensor(113)\n","tensor(38)\n","tensor(134)\n","tensor(44)\n","tensor(119)\n","tensor(98)\n","tensor(88)\n","tensor(40)\n","tensor(18)\n","tensor(429)\n","tensor(1336)\n","tensor(2034)\n","tensor(2295)\n","tensor(286)\n","tensor(588)\n","tensor(698)\n","tensor(911)\n","tensor(1051)\n","tensor(2040)\n","tensor(2119)\n","tensor(2120)\n","tensor(2121)\n","tensor(1103)\n","tensor(1358)\n","tensor(1739)\n","tensor(412)\n","tensor(447)\n","tensor(1616)\n","tensor(1849)\n","tensor(457)\n","tensor(710)\n","tensor(1392)\n","tensor(2213)\n","tensor(2214)\n","tensor(2215)\n","tensor(861)\n","tensor(693)\n","tensor(734)\n","tensor(1535)\n","tensor(2054)\n","tensor(736)\n","tensor(456)\n","tensor(1303)\n","tensor(2181)\n","tensor(2074)\n","tensor(2183)\n","tensor(2197)\n","tensor(1628)\n","tensor(2200)\n","tensor(2201)\n","tensor(279)\n","tensor(518)\n","tensor(1850)\n","tensor(1973)\n","tensor(306)\n","tensor(487)\n","tensor(1623)\n","tensor(2080)\n","tensor(1139)\n","tensor(1467)\n","tensor(2053)\n","tensor(2172)\n","tensor(2182)\n","tensor(1520)\n","tensor(2581)\n","tensor(284)\n","tensor(2223)\n","tensor(2224)\n","tensor(2226)\n","tensor(259)\n","tensor(2537)\n","tensor(483)\n","tensor(514)\n","tensor(816)\n","tensor(1842)\n","tensor(2405)\n","tensor(236)\n","tensor(1776)\n","tensor(208)\n","tensor(152)\n","tensor(963)\n","tensor(1530)\n","tensor(1653)\n","tensor(2399)\n","tensor(2400)\n","tensor(210)\n","tensor(323)\n","tensor(651)\n","tensor(771)\n","tensor(787)\n","tensor(815)\n","tensor(1079)\n","tensor(1156)\n","tensor(1983)\n","tensor(2020)\n","tensor(2021)\n","tensor(831)\n","tensor(391)\n","tensor(1986)\n","tensor(583)\n","tensor(2222)\n","tensor(2225)\n","tensor(109)\n","tensor(1251)\n","tensor(1448)\n","tensor(1561)\n","tensor(1871)\n","tensor(1878)\n","tensor(2256)\n","tensor(973)\n","tensor(158)\n","tensor(2075)\n","tensor(2077)\n","tensor(2668)\n","tensor(1349)\n","tensor(1522)\n","tensor(1532)\n","tensor(1634)\n","tensor(1965)\n","tensor(2357)\n","tensor(1604)\n","tensor(2366)\n","tensor(175)\n","tensor(596)\n","tensor(644)\n","tensor(1914)\n","tensor(269)\n","tensor(281)\n","tensor(1996)\n","tensor(2604)\n","tensor(1594)\n","tensor(898)\n","tensor(1836)\n","tensor(834)\n","tensor(2169)\n","tensor(1441)\n","tensor(2461)\n","tensor(137)\n","tensor(2095)\n","tensor(2144)\n","tensor(1219)\n","tensor(1329)\n","tensor(1418)\n","tensor(257)\n","tensor(1117)\n","tensor(2049)\n","tensor(660)\n","tensor(910)\n","tensor(1780)\n","tensor(2045)\n","tensor(484)\n","tensor(608)\n","tensor(1775)\n","tensor(1790)\n","tensor(485)\n","tensor(2418)\n","tensor(26)\n","tensor(123)\n","tensor(2454)\n","tensor(2455)\n","tensor(105)\n","tensor(580)\n","tensor(609)\n","tensor(615)\n","tensor(1067)\n","tensor(1287)\n","tensor(1627)\n","tensor(1725)\n","tensor(2651)\n","tensor(239)\n","tensor(543)\n","tensor(619)\n","tensor(1293)\n","tensor(2419)\n","tensor(589)\n","tensor(802)\n","tensor(980)\n","tensor(1158)\n","tensor(1910)\n","tensor(2251)\n","tensor(2252)\n","tensor(2544)\n","tensor(1367)\n","tensor(1622)\n","tensor(2478)\n","tensor(37)\n","tensor(1527)\n","tensor(399)\n","tensor(452)\n","tensor(1670)\n","tensor(1777)\n","tensor(1784)\n","tensor(2082)\n","tensor(258)\n","tensor(884)\n","tensor(1087)\n","tensor(1094)\n","tensor(1157)\n","tensor(1401)\n","tensor(1585)\n","tensor(2463)\n","tensor(1647)\n","tensor(2157)\n","tensor(2209)\n","tensor(1602)\n","tensor(2056)\n","tensor(1234)\n","tensor(1702)\n","tensor(1703)\n","tensor(2238)\n","tensor(1138)\n","tensor(970)\n","tensor(1632)\n","tensor(2444)\n","tensor(2642)\n","tensor(633)\n","tensor(1862)\n","tensor(2582)\n","tensor(401)\n","tensor(767)\n","tensor(1072)\n","tensor(2269)\n","tensor(2270)\n","tensor(2374)\n","tensor(2375)\n","tensor(723)\n","tensor(2614)\n","tensor(603)\n","tensor(2097)\n","tensor(540)\n","tensor(747)\n","tensor(1884)\n","tensor(862)\n","tensor(863)\n","tensor(1160)\n","tensor(406)\n","tensor(1582)\n","tensor(2624)\n","tensor(2701)\n","tensor(379)\n","tensor(646)\n","tensor(1537)\n","tensor(1549)\n","tensor(1901)\n","tensor(1959)\n","tensor(1521)\n","tensor(1713)\n","tensor(1847)\n","tensor(1394)\n","tensor(300)\n","tensor(1908)\n","tensor(1741)\n","tensor(1309)\n","tensor(1494)\n","tensor(2014)\n","tensor(2015)\n","tensor(415)\n","tensor(1732)\n","tensor(2013)\n","tensor(1882)\n","tensor(507)\n","tensor(866)\n","tensor(1364)\n","tensor(1560)\n","tensor(1786)\n","tensor(2145)\n","tensor(1580)\n","tensor(2017)\n","tensor(130)\n","tensor(2012)\n","tensor(841)\n","tensor(851)\n","tensor(696)\n","tensor(2394)\n","tensor(1669)\n","tensor(1618)\n","tensor(1807)\n","tensor(2041)\n","tensor(794)\n","tensor(2001)\n","tensor(2043)\n","tensor(2044)\n","tensor(695)\n","tensor(1012)\n","tensor(1452)\n","tensor(1797)\n","tensor(2032)\n","tensor(2033)\n","tensor(1026)\n","tensor(1654)\n","tensor(2037)\n","tensor(691)\n","tensor(791)\n","tensor(1894)\n","tensor(1929)\n","tensor(2042)\n","tensor(897)\n","tensor(1110)\n","tensor(1655)\n","tensor(2155)\n","tensor(442)\n","tensor(2383)\n","tensor(1015)\n","tensor(893)\n","tensor(1534)\n","tensor(2376)\n","tensor(2122)\n","tensor(218)\n","tensor(665)\n","tensor(2637)\n","tensor(1480)\n","tensor(1760)\n","tensor(998)\n","tensor(1284)\n","tensor(748)\n","tensor(1887)\n","tensor(1499)\n","tensor(1123)\n","tensor(1710)\n","tensor(1758)\n","tensor(1720)\n","tensor(687)\n","tensor(1149)\n","tensor(1169)\n","tensor(1759)\n","tensor(616)\n","tensor(1317)\n","tensor(1709)\n","tensor(1750)\n","tensor(1766)\n","tensor(1886)\n","tensor(2221)\n","tensor(2638)\n","tensor(118)\n","tensor(539)\n","tensor(581)\n","tensor(1402)\n","tensor(2050)\n","tensor(1013)\n","tensor(1487)\n","tensor(1201)\n","tensor(2216)\n","tensor(2212)\n","tensor(421)\n","tensor(1724)\n","tensor(1397)\n","tensor(2068)\n","tensor(2330)\n","tensor(334)\n","tensor(763)\n","tensor(1851)\n","tensor(2108)\n","tensor(964)\n","tensor(408)\n","tensor(1388)\n","tensor(1841)\n","tensor(1006)\n","tensor(2202)\n","tensor(2277)\n","tensor(280)\n","tensor(1351)\n","tensor(1916)\n","tensor(2293)\n","tensor(525)\n","tensor(1042)\n","tensor(1540)\n","tensor(2133)\n","tensor(751)\n","tensor(1631)\n","tensor(142)\n","tensor(544)\n","tensor(667)\n","tensor(2199)\n","tensor(2347)\n","tensor(2472)\n","tensor(746)\n","tensor(979)\n","tensor(2163)\n","tensor(2196)\n","tensor(2180)\n","tensor(965)\n","tensor(1635)\n","tensor(2055)\n","tensor(2057)\n","tensor(552)\n","tensor(2280)\n","tensor(242)\n","tensor(304)\n","tensor(270)\n","tensor(666)\n","tensor(2165)\n","tensor(1195)\n","tensor(2344)\n","tensor(2423)\n","tensor(1373)\n","tensor(1974)\n","tensor(779)\n","tensor(1333)\n","tensor(490)\n","tensor(252)\n","tensor(1525)\n","tensor(1975)\n","tensor(1482)\n","tensor(1640)\n","tensor(573)\n","tensor(1705)\n","tensor(153)\n","tensor(1782)\n","tensor(2088)\n","tensor(1787)\n","tensor(655)\n","tensor(859)\n","tensor(426)\n","tensor(1772)\n","tensor(2026)\n","tensor(1798)\n","tensor(556)\n","tensor(1779)\n","tensor(1144)\n","tensor(542)\n","tensor(314)\n","tensor(1245)\n","tensor(1311)\n","tensor(1459)\n","tensor(1770)\n","tensor(858)\n","tensor(2231)\n","tensor(2198)\n","tensor(2232)\n","tensor(2233)\n","tensor(1765)\n","tensor(449)\n","tensor(436)\n","tensor(478)\n","tensor(1314)\n","tensor(1978)\n","tensor(1980)\n","tensor(2282)\n","tensor(1203)\n","tensor(1538)\n","tensor(1979)\n","tensor(2281)\n","tensor(2046)\n","tensor(126)\n","tensor(1572)\n","tensor(2078)\n","tensor(2079)\n","tensor(337)\n","tensor(960)\n","tensor(1283)\n","tensor(1735)\n","tensor(2324)\n","tensor(2240)\n","tensor(29)\n","tensor(1141)\n","tensor(1443)\n","tensor(805)\n","tensor(375)\n","tensor(2401)\n","tensor(1369)\n","tensor(2464)\n","tensor(2434)\n","tensor(164)\n","tensor(1614)\n","tensor(1626)\n","tensor(1671)\n","tensor(1905)\n","tensor(1906)\n","tensor(1907)\n","tensor(2309)\n","tensor(162)\n","tensor(498)\n","tensor(2022)\n","tensor(885)\n","tensor(1667)\n","tensor(1984)\n","tensor(2023)\n","tensor(2024)\n","tensor(2156)\n","tensor(1080)\n","tensor(325)\n","tensor(438)\n","tensor(1174)\n","tensor(1344)\n","tensor(2018)\n","tensor(1218)\n","tensor(741)\n","tensor(1011)\n","tensor(1638)\n","tensor(1733)\n","tensor(2093)\n","tensor(493)\n","tensor(2365)\n","tensor(519)\n","tensor(968)\n","tensor(681)\n","tensor(1994)\n","tensor(673)\n","tensor(1997)\n","tensor(1998)\n","tensor(2004)\n","tensor(1873)\n","tensor(151)\n","tensor(289)\n","tensor(1337)\n","tensor(1789)\n","tensor(2092)\n","tensor(1661)\n","tensor(505)\n","tensor(228)\n","tensor(417)\n","tensor(1701)\n","tensor(661)\n","tensor(2016)\n","tensor(630)\n","tensor(176)\n","tensor(1583)\n","tensor(454)\n","tensor(935)\n","tensor(671)\n","tensor(1142)\n","tensor(2019)\n","tensor(2153)\n","tensor(180)\n","tensor(775)\n","tensor(1020)\n","tensor(2076)\n","tensor(2091)\n","tensor(2667)\n","tensor(486)\n","tensor(1704)\n","tensor(45)\n","tensor(1320)\n","tensor(2684)\n","tensor(1697)\n","tensor(2680)\n","tensor(2648)\n","tensor(1050)\n","tensor(1461)\n","tensor(2393)\n","tensor(1964)\n","tensor(94)\n","tensor(1846)\n","tensor(2355)\n","tensor(2356)\n","tensor(902)\n","tensor(686)\n","tensor(1738)\n","tensor(955)\n","tensor(2135)\n","tensor(2217)\n","tensor(2388)\n","tensor(496)\n","tensor(992)\n","tensor(711)\n","tensor(728)\n","tensor(1258)\n","tensor(2177)\n","tensor(506)\n","tensor(921)\n","tensor(604)\n","tensor(1676)\n","tensor(1915)\n","tensor(1675)\n","tensor(1464)\n","tensor(1917)\n","tensor(321)\n","tensor(418)\n","tensor(2543)\n","tensor(2551)\n","tensor(101)\n","tensor(1000)\n","tensor(1347)\n","tensor(1382)\n","tensor(2244)\n","tensor(2247)\n","tensor(327)\n","tensor(2063)\n","tensor(2064)\n","tensor(1835)\n","tensor(2160)\n","tensor(683)\n","tensor(876)\n","tensor(2168)\n","tensor(1546)\n","tensor(1547)\n","tensor(2272)\n","tensor(371)\n","tensor(1707)\n","tensor(1619)\n","tensor(1958)\n","tensor(2271)\n","tensor(1060)\n","tensor(2275)\n","tensor(2329)\n","tensor(2504)\n","tensor(2331)\n","tensor(994)\n","tensor(733)\n","tensor(434)\n","tensor(2499)\n","tensor(887)\n","tensor(2085)\n","tensor(1624)\n","tensor(1773)\n","tensor(1791)\n","tensor(318)\n","tensor(699)\n","tensor(1830)\n","tensor(1699)\n","tensor(2081)\n","tensor(191)\n","tensor(350)\n","tensor(1609)\n","tensor(839)\n","tensor(2219)\n","tensor(572)\n","tensor(1721)\n","tensor(2476)\n","tensor(744)\n","tensor(1742)\n","tensor(2445)\n","tensor(534)\n","tensor(883)\n","tensor(1041)\n","tensor(2446)\n","tensor(2447)\n","tensor(2448)\n","tensor(2443)\n","tensor(170)\n","tensor(936)\n","tensor(1216)\n","tensor(2307)\n","tensor(481)\n","tensor(1164)\n","tensor(1229)\n","tensor(1308)\n","tensor(1740)\n","tensor(628)\n","tensor(2334)\n","tensor(1070)\n","tensor(1745)\n","tensor(1734)\n","tensor(1427)\n","tensor(2413)\n","tensor(1712)\n","tensor(2597)\n","tensor(1069)\n","tensor(1274)\n","tensor(1220)\n","tensor(1376)\n","tensor(1909)\n","tensor(984)\n","tensor(74)\n","tensor(497)\n","tensor(1848)\n","tensor(1307)\n","tensor(302)\n","tensor(814)\n","tensor(2382)\n","tensor(1296)\n","tensor(1196)\n","tensor(1584)\n","tensor(1656)\n","tensor(1778)\n","tensor(2143)\n","tensor(1270)\n","tensor(1289)\n","tensor(2425)\n","tensor(1190)\n","tensor(2427)\n","tensor(331)\n","tensor(860)\n","tensor(1668)\n","tensor(2274)\n","tensor(2450)\n","tensor(1089)\n","tensor(1769)\n","tensor(2107)\n","tensor(303)\n","tensor(308)\n","tensor(1346)\n","tensor(2096)\n","tensor(1153)\n","tensor(1240)\n","tensor(2645)\n","tensor(761)\n","tensor(340)\n","tensor(584)\n","tensor(1504)\n","tensor(2158)\n","tensor(2159)\n","tensor(6)\n","tensor(315)\n","tensor(1204)\n","tensor(1416)\n","tensor(2072)\n","tensor(2073)\n","tensor(1680)\n","tensor(2311)\n","tensor(2576)\n","tensor(1966)\n","tensor(463)\n","tensor(1365)\n","tensor(1970)\n","tensor(1971)\n","tensor(759)\n","tensor(1969)\n","tensor(627)\n","tensor(706)\n","tensor(2239)\n","tensor(2248)\n","tensor(2250)\n","tensor(364)\n","tensor(466)\n","tensor(1756)\n","tensor(464)\n","tensor(2503)\n","tensor(358)\n","tensor(1866)\n","tensor(926)\n","tensor(1166)\n","tensor(104)\n","tensor(1065)\n","tensor(1096)\n","tensor(873)\n","tensor(2314)\n","tensor(1262)\n","tensor(945)\n","tensor(1802)\n","tensor(1801)\n","tensor(244)\n","tensor(370)\n","tensor(392)\n","tensor(1415)\n","tensor(1903)\n","tensor(128)\n","tensor(1354)\n","tensor(1414)\n","tensor(1404)\n","tensor(494)\n","tensor(1599)\n","tensor(2442)\n","tensor(716)\n","tensor(795)\n","tensor(1248)\n","tensor(1821)\n","tensor(522)\n","tensor(975)\n","tensor(2098)\n","tensor(67)\n","tensor(2527)\n","tensor(282)\n","tensor(249)\n","tensor(854)\n","tensor(1885)\n","tensor(1590)\n","tensor(2268)\n","tensor(2505)\n","tensor(132)\n","tensor(904)\n","tensor(1022)\n","tensor(1898)\n","tensor(441)\n","tensor(1455)\n","tensor(2286)\n","tensor(645)\n","tensor(1954)\n","tensor(2284)\n","tensor(2621)\n","tensor(1696)\n","tensor(211)\n","tensor(2137)\n","tensor(634)\n","tensor(1178)\n","tensor(1474)\n","tensor(1529)\n","tensor(1677)\n","tensor(2593)\n","tensor(995)\n","tensor(61)\n","tensor(1625)\n","tensor(2071)\n","tensor(818)\n","tensor(737)\n","tensor(204)\n","tensor(1224)\n","tensor(1566)\n","tensor(471)\n","tensor(516)\n","tensor(253)\n","tensor(219)\n","tensor(1300)\n","tensor(251)\n","tensor(1363)\n","tensor(1933)\n","tensor(1211)\n","tensor(1936)\n","tensor(1413)\n","tensor(1940)\n","tensor(1049)\n","tensor(1564)\n","tensor(1512)\n","tensor(523)\n","tensor(1891)\n","tensor(49)\n","tensor(141)\n","tensor(1748)\n","tensor(34)\n","tensor(1339)\n","tensor(597)\n","tensor(1637)\n","tensor(1804)\n","tensor(2087)\n","tensor(1783)\n","tensor(1838)\n","tensor(2131)\n","tensor(1282)\n","tensor(203)\n","tensor(1875)\n","tensor(792)\n","tensor(1990)\n","tensor(682)\n","tensor(830)\n","tensor(2273)\n","tensor(1700)\n","tensor(563)\n","tensor(1483)\n","tensor(342)\n","tensor(958)\n","tensor(1288)\n","tensor(2178)\n","tensor(1843)\n","tensor(254)\n","tensor(1176)\n","tensor(2508)\n","tensor(1892)\n","tensor(2467)\n","tensor(1982)\n","tensor(272)\n","tensor(277)\n","tensor(571)\n","tensor(1570)\n","tensor(1027)\n","tensor(1421)\n","tensor(1420)\n","tensor(504)\n","tensor(2396)\n","tensor(1481)\n","tensor(2332)\n","tensor(1143)\n","tensor(1425)\n","tensor(482)\n","tensor(962)\n","tensor(48)\n","tensor(788)\n","tensor(2123)\n","tensor(574)\n","tensor(91)\n","tensor(2406)\n","tensor(407)\n","tensor(1681)\n","tensor(948)\n","tensor(1682)\n","tensor(1417)\n","tensor(740)\n","tensor(1002)\n","tensor(916)\n","tensor(2531)\n","tensor(835)\n","tensor(1567)\n","tensor(1952)\n","tensor(2130)\n","tensor(261)\n","tensor(1405)\n","tensor(770)\n","tensor(2117)\n","tensor(2118)\n","tensor(1268)\n","tensor(1927)\n","tensor(2039)\n","tensor(1410)\n","tensor(1152)\n","tensor(1120)\n","tensor(2025)\n","tensor(1839)\n","tensor(2384)\n","tensor(1131)\n","tensor(749)\n","tensor(196)\n","tensor(924)\n","tensor(2263)\n","tensor(1788)\n","tensor(1068)\n","tensor(638)\n","tensor(1082)\n","tensor(1428)\n","tensor(1429)\n","tensor(1519)\n","tensor(813)\n","tensor(783)\n","tensor(1202)\n","tensor(2066)\n","tensor(2594)\n","tensor(781)\n","tensor(332)\n","tensor(397)\n","tensor(576)\n","tensor(976)\n","tensor(978)\n","tensor(1431)\n","tensor(757)\n","tensor(2553)\n","tensor(1145)\n","tensor(1888)\n","tensor(1214)\n","tensor(372)\n","tensor(480)\n","tensor(2466)\n","tensor(1033)\n","tensor(1610)\n","tensor(985)\n","tensor(301)\n","tensor(2492)\n","tensor(1714)\n","tensor(1731)\n","tensor(689)\n","tensor(569)\n","tensor(2220)\n","tensor(796)\n","tensor(1601)\n","tensor(823)\n","tensor(1148)\n","tensor(2010)\n","tensor(842)\n","tensor(554)\n","tensor(2112)\n","tensor(388)\n","tensor(1792)\n","tensor(1163)\n","tensor(1919)\n","tensor(1972)\n","tensor(2485)\n","tensor(530)\n","tensor(1403)\n","tensor(1501)\n","tensor(2533)\n","tensor(708)\n","tensor(768)\n","tensor(1723)\n","tensor(1330)\n","tensor(1377)\n","tensor(2203)\n","tensor(707)\n","tensor(347)\n","tensor(1302)\n","tensor(262)\n","tensor(907)\n","tensor(2134)\n","tensor(69)\n","tensor(2333)\n","tensor(1025)\n","tensor(276)\n","tensor(1695)\n","tensor(1463)\n","tensor(2364)\n","tensor(731)\n","tensor(1692)\n","tensor(192)\n","tensor(591)\n","tensor(642)\n","tensor(1017)\n","tensor(2006)\n","tensor(524)\n","tensor(1957)\n","tensor(2207)\n","tensor(1073)\n","tensor(1348)\n","tensor(1502)\n","tensor(2162)\n","tensor(81)\n","tensor(1693)\n","tensor(1588)\n","tensor(838)\n","tensor(1172)\n","tensor(1857)\n","tensor(1215)\n","tensor(1343)\n","tensor(868)\n","tensor(1058)\n","tensor(2343)\n","tensor(2475)\n","tensor(2151)\n","tensor(848)\n","tensor(1003)\n","tensor(578)\n","tensor(778)\n","tensor(1592)\n","tensor(1370)\n","tensor(2195)\n","tensor(2312)\n","tensor(2105)\n","tensor(826)\n","tensor(1981)\n","tensor(2109)\n","tensor(2132)\n","tensor(36)\n","tensor(643)\n","tensor(1313)\n","tensor(2407)\n","tensor(1193)\n","tensor(2322)\n","tensor(459)\n","tensor(1810)\n","tensor(762)\n","tensor(1030)\n","tensor(1199)\n","tensor(1805)\n","tensor(329)\n","tensor(2000)\n","tensor(468)\n","tensor(1312)\n","tensor(2218)\n","tensor(2502)\n","tensor(2292)\n","tensor(636)\n","tensor(2291)\n","tensor(1711)\n","tensor(501)\n","tensor(1719)\n","tensor(73)\n","tensor(1280)\n","tensor(1834)\n","tensor(878)\n","tensor(1977)\n","tensor(890)\n","tensor(1269)\n","tensor(621)\n","tensor(2164)\n","tensor(424)\n","tensor(461)\n","tensor(382)\n","tensor(1500)\n","tensor(2185)\n","tensor(2048)\n","tensor(1551)\n","tensor(836)\n","tensor(784)\n","tensor(2650)\n","tensor(969)\n","tensor(562)\n","tensor(2294)\n","tensor(1350)\n","tensor(275)\n","tensor(2236)\n","tensor(2435)\n","tensor(1207)\n","tensor(1795)\n","tensor(444)\n","tensor(828)\n","tensor(1968)\n","tensor(2136)\n","tensor(2059)\n","tensor(674)\n","tensor(566)\n","tensor(1399)\n","tensor(1869)\n","tensor(356)\n","tensor(437)\n","tensor(1652)\n","tensor(2190)\n","tensor(2386)\n","tensor(1266)\n","tensor(1332)\n","tensor(324)\n","tensor(149)\n","tensor(697)\n","tensor(738)\n","tensor(718)\n","tensor(1985)\n","tensor(1912)\n","tensor(25)\n","tensor(724)\n","tensor(1928)\n","tensor(2315)\n","tensor(76)\n","tensor(586)\n","tensor(2116)\n","tensor(1088)\n","tensor(1244)\n","tensor(416)\n","tensor(387)\n","tensor(598)\n","tensor(1603)\n","tensor(1171)\n","tensor(1445)\n","tensor(1446)\n","tensor(169)\n","tensor(2412)\n","tensor(1781)\n","tensor(1197)\n","tensor(316)\n","tensor(1987)\n","tensor(1820)\n","tensor(2335)\n","tensor(1722)\n","tensor(1180)\n","tensor(322)\n","tensor(2494)\n","tensor(205)\n","tensor(1799)\n","tensor(24)\n","tensor(13)\n","tensor(97)\n","tensor(1879)\n","tensor(344)\n","tensor(807)\n","tensor(231)\n","tensor(376)\n","tensor(1127)\n","tensor(1995)\n","tensor(2189)\n","tensor(451)\n","tensor(2326)\n","tensor(790)\n","tensor(171)\n","tensor(435)\n","tensor(383)\n","tensor(2303)\n","tensor(1491)\n","tensor(1119)\n","tensor(2253)\n","tensor(243)\n","tensor(2473)\n","tensor(2564)\n","tensor(1081)\n","tensor(1569)\n","tensor(368)\n","tensor(248)\n","tensor(1649)\n","tensor(2490)\n","tensor(675)\n","tensor(1867)\n","tensor(934)\n","tensor(2211)\n","tensor(190)\n","tensor(96)\n","tensor(551)\n","tensor(2111)\n","tensor(1285)\n","tensor(398)\n","tensor(2367)\n","tensor(2371)\n","tensor(1352)\n","tensor(1918)\n","tensor(1074)\n","tensor(2649)\n","tensor(181)\n","tensor(850)\n","tensor(2457)\n","tensor(1083)\n","tensor(1450)\n","tensor(1589)\n","tensor(972)\n","tensor(1325)\n","tensor(2176)\n","tensor(2283)\n","tensor(2243)\n","tensor(2246)\n","tensor(928)\n","tensor(1326)\n","tensor(1328)\n","tensor(1698)\n","tensor(2584)\n","tensor(1726)\n","tensor(1586)\n","tensor(341)\n","tensor(720)\n","tensor(1553)\n","tensor(735)\n","tensor(1237)\n","tensor(1881)\n","tensor(739)\n","tensor(1128)\n","tensor(579)\n","tensor(1192)\n","tensor(2011)\n","tensor(1524)\n","tensor(2408)\n","tensor(1490)\n","tensor(2089)\n","tensor(384)\n","tensor(1045)\n","tensor(2327)\n","tensor(889)\n","tensor(1513)\n","tensor(1505)\n","tensor(2385)\n","tensor(656)\n","tensor(2090)\n","tensor(475)\n","tensor(1687)\n","tensor(1761)\n","tensor(564)\n","tensor(1596)\n","tensor(223)\n","tensor(1154)\n","tensor(526)\n","tensor(174)\n","tensor(227)\n","tensor(629)\n","tensor(933)\n","tensor(1243)\n","tensor(1466)\n","tensor(2390)\n","tensor(1716)\n","tensor(1037)\n","tensor(246)\n","tensor(2515)\n","tensor(2530)\n","tensor(2641)\n","tensor(1384)\n","tensor(871)\n","tensor(2440)\n","tensor(2441)\n","tensor(1506)\n","tensor(1254)\n","tensor(1035)\n","tensor(2389)\n","tensor(1764)\n","tensor(2546)\n","tensor(1829)\n","tensor(2414)\n","tensor(2415)\n","tensor(915)\n","tensor(1389)\n","tensor(2305)\n","tensor(2265)\n","tensor(2052)\n","tensor(804)\n","tensor(922)\n","tensor(1444)\n","tensor(2536)\n","tensor(719)\n","tensor(1253)\n","tensor(35)\n","tensor(1911)\n","tensor(440)\n","tensor(895)\n","tensor(943)\n","tensor(1061)\n","tensor(2300)\n","tensor(1552)\n","tensor(1771)\n","tensor(1341)\n","tensor(1526)\n","tensor(2099)\n","tensor(2426)\n","tensor(1184)\n","tensor(1387)\n","tensor(349)\n","tensor(2428)\n","tensor(2480)\n","tensor(355)\n","tensor(389)\n","tensor(1412)\n","tensor(704)\n","tensor(2086)\n","tensor(1009)\n","tensor(937)\n","tensor(1856)\n","tensor(2237)\n","tensor(623)\n","tensor(2289)\n","tensor(23)\n","tensor(373)\n","tensor(423)\n","tensor(1921)\n","tensor(1925)\n","tensor(1924)\n","tensor(1395)\n","tensor(901)\n","tensor(443)\n","tensor(1010)\n","tensor(2509)\n","tensor(2519)\n","tensor(271)\n","tensor(2249)\n","tensor(1071)\n","tensor(428)\n","tensor(856)\n","tensor(1545)\n","tensor(283)\n","tensor(348)\n","tensor(1260)\n","tensor(732)\n","tensor(1606)\n","tensor(154)\n","tensor(1305)\n","tensor(650)\n","tensor(403)\n","tensor(864)\n","tensor(2210)\n","tensor(85)\n","tensor(411)\n","tensor(1633)\n","tensor(1479)\n","tensor(2313)\n","tensor(1708)\n","tensor(110)\n","tensor(567)\n","tensor(1509)\n","tensor(2451)\n","tensor(565)\n","tensor(1408)\n","tensor(1825)\n","tensor(709)\n","tensor(840)\n","tensor(1902)\n","tensor(2592)\n","tensor(233)\n","tensor(400)\n","tensor(1183)\n","tensor(2460)\n","tensor(657)\n","tensor(867)\n","tensor(2409)\n","tensor(1645)\n","tensor(1651)\n","tensor(2628)\n","tensor(857)\n","tensor(1097)\n","tensor(470)\n","tensor(2644)\n","tensor(309)\n","tensor(1278)\n","tensor(479)\n","tensor(541)\n","tensor(1961)\n","tensor(1276)\n","tensor(755)\n","tensor(193)\n","tensor(2094)\n","tensor(2285)\n","tensor(1824)\n","tensor(891)\n","tensor(1899)\n","tensor(1495)\n","tensor(2227)\n","tensor(2350)\n","tensor(593)\n","tensor(2261)\n","tensor(1729)\n","tensor(725)\n","tensor(1840)\n","tensor(1999)\n","tensor(1630)\n","tensor(1895)\n","tensor(2104)\n","tensor(1542)\n","tensor(877)\n","tensor(1941)\n","tensor(812)\n","tensor(402)\n","tensor(1932)\n","tensor(1935)\n","tensor(776)\n","tensor(1934)\n","tensor(1432)\n","tensor(2610)\n"]}],"source":["for i in n_id:\n","    print(i)"]},{"cell_type":"markdown","metadata":{},"source":["### SAGEConv"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class SAGE(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n","        super().__init__()\n","\n","        self.num_layers = num_layers\n","\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(SAGEConv(in_channels, hidden_channels))\n","        for _ in range(num_layers - 2):\n","            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n","        self.convs.append(SAGEConv(hidden_channels, out_channels))\n","\n","    def reset_parameters(self):\n","        for conv in self.convs:\n","            conv.reset_parameters()\n","\n","    def forward(self, x, adjs):\n","        # `train_loader` computes the k-hop neighborhood of a batch of nodes,\n","        # and returns, for each layer, a bipartite graph object, holding the\n","        # bipartite edges `edge_index`, the index `e_id` of the original edges,\n","        # and the size/shape `size` of the bipartite graph.\n","        # Target nodes are also included in the source nodes so that one can\n","        # easily apply skip-connections or add self-loops.\n","        for i, (edge_index, _, size) in enumerate(adjs):\n","            # 对每一层的bipartite图都有x_target = x[:size[1]]\n","            x_target = x[:size[1]]  # Target nodes are always placed first.目标节点放在最前面，一共有size[1]个目标节点\n","            # 实现了对一层bipartite图的卷积。可以把卷积就理解为聚合操作，这里就是逐层聚合，从第L层到第1层\n","            x = self.convs[i]((x, x_target), edge_index)\n","            if i != self.num_layers - 1:  # 不是最后一层就执行下面的操作\n","                x = F.relu(x)\n","                x = F.dropout(x, p=0.5, training=self.training)\n","        return x.log_softmax(dim=-1)\n","\n","    def inference(self, x_all):\n","        pbar = tqdm(total=x_all.size(0) * self.num_layers)\n","        pbar.set_description('Evaluating')\n","\n","        # Compute representations of nodes layer by layer, using *all*\n","        # available edges. This leads to faster computation in contrast to\n","        # immediately computing the final representations of each batch.\n","        total_edges = 0\n","        for i in range(self.num_layers): # 一共有l层\n","            xs = []\n","            # 一个batchsize中的目标节点采样L=1层涉及到的所有节点\n","            for batch_size, n_id, adj in subgraph_loader:\n","                edge_index, _, size = adj.to(device)\n","                total_edges += edge_index.size(1)\n","                x = x_all[n_id].to(device)\n","                x_target = x[:size[1]]\n","                x = self.convs[i]((x, x_target), edge_index)\n","                if i != self.num_layers - 1:\n","                    x = F.relu(x)\n","                xs.append(x.cpu())\n","\n","                pbar.update(batch_size)\n","\n","            x_all = torch.cat(xs, dim=0)\n","\n","        pbar.close()\n","\n","        return x_all"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = SAGE(dataset.num_features, 64, dataset.num_classes, num_layers=3)\n","model = model.to(device)\n","\n","x = data.x.to(device)\n","y = data.y.squeeze().to(device)\n","criterion = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def train(epoch):\n","    model.train()\n","\n","    pbar = tqdm(total=train_idx.size(0))\n","    pbar.set_description(f'Epoch {epoch:02d}')\n","\n","    total_loss = total_correct = 0\n","\n","    for batch_size, n_id, adjs in train_loader:\n","        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n","        adjs = [adj.to(device) for adj in adjs]\n","\n","        optimizer.zero_grad()\n","        out = model(x[n_id], adjs)  # x[n_id]这个batchsize中的目标节点采样L层涉及到的所有节点\n","        loss = criterion(out, y[n_id[:batch_size]])\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += float(loss)\n","        total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())\n","        pbar.update(batch_size)\n","\n","    pbar.close()\n","\n","    loss = total_loss / len(train_loader)\n","    approx_acc = total_correct / train_idx.size(0)\n","\n","    return loss, approx_acc"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["@torch.no_grad()\n","def test():\n","    model.eval()\n","\n","    out = model.inference(x)\n","\n","    y_true = y.cpu().unsqueeze(-1)\n","    y_pred = out.argmax(dim=-1, keepdim=True)\n","    correct = (y_pred == y_true).sum().item()\n","    test_acc = correct/data.num_nodes\n","    return test_acc"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Run 01:\n","\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 01: 100%|██████████| 140/140 [00:10<00:00, 13.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 01, Loss: 1.9410, Approx. Train: 0.1429\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 02: 100%|██████████| 140/140 [00:09<00:00, 14.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 02, Loss: 1.8810, Approx. Train: 0.3643\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 03: 100%|██████████| 140/140 [00:10<00:00, 13.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 03, Loss: 1.7864, Approx. Train: 0.6500\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 04: 100%|██████████| 140/140 [00:10<00:00, 13.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 04, Loss: 1.6264, Approx. Train: 0.8286\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 05: 100%|██████████| 140/140 [00:10<00:00, 13.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 05, Loss: 1.4584, Approx. Train: 0.8214\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 06: 100%|██████████| 140/140 [00:10<00:00, 12.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 06, Loss: 1.2353, Approx. Train: 0.8143\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 07: 100%|██████████| 140/140 [00:10<00:00, 13.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 07, Loss: 0.9877, Approx. Train: 0.8857\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 08: 100%|██████████| 140/140 [00:11<00:00, 12.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 08, Loss: 0.7243, Approx. Train: 0.9214\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 09: 100%|██████████| 140/140 [00:12<00:00, 11.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 09, Loss: 0.4996, Approx. Train: 0.9286\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 8124/8124 [00:32<00:00, 246.61it/s]"]},{"name":"stdout","output_type":"stream","text":["Test: 0.8002\n","============================\n","Final Test: 0.8002 ± nan\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["test_accs = []\n","for run in range(1, 2):#11\n","    print('')\n","    print(f'Run {run:02d}:')\n","    print('')\n","\n","    model.reset_parameters()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n","\n","    best_val_acc = final_test_acc = 0\n","    for epoch in range(1, 10):#51\n","        loss, acc = train(epoch)\n","        print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}')\n","\n","\n","    test_acc = test()\n","    print(f'Test: {test_acc:.4f}')\n","    test_accs.append(test_acc)\n","\n","test_acc = torch.tensor(test_accs)\n","print('============================')\n","print(f'Final Test: {test_acc.mean():.4f} ± {test_acc.std():.4f}')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
